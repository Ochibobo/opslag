## Storage Engine

This document describes the implementation of the `Lignum Storage Engine (OsE)`. This spans from disk storage to memory management using a
buffer pool manager. This document describes the following:

- [ ] The database structure
  - [ ] Each database instance is a folder
  - [ ] Each table is a file
  - [ ] Each index is a file too
- [ ] The storage model
  - [ ] Lignum is a Key/Value store
  - [ ] It uses the PAX storage model; a combination of row-oriented & column-oriented storage model
- [ ] The file organization structurer
  - [ ] Heap file organization
  - [ ] Tree file organization
  - [ ] Sequential/Sorted File organization
  - [ ] Hashing file organization
- [ ] The pages:
  - [ ] The page id
    - [ ] It's structure:
      - [ ] Database instance
      - [ ] Table
  - [ ] The page size
    - [ ] Read heavy vs write heavy
  - [ ] The page directory
  - [ ] The page header
  - [ ] Data pages
  - [ ] Index pages
  - [ ] Page Layout:
    - [ ] Tuple based:
      - [ ] Slotted pages
      - [ ] Record ID - used by the database not the user space
        - [ ] File ID, Page ID, Slot #
        - [ ] An offset page for variable length data
    - [ ] Layout
      - [ ] Convert types to bytes
      - [ ] Lignum is responsible to interpret bytes back to attributes & values
      - [ ] Tuples have a header describing their content
      - [ ] Tuple data
        - [ ] Determine bytes layout
          - [ ] Padding if needed
          - [ ] `C` or `Rust` like memory layout
    - [ ] Column based
    - [ ] Index based
  - [ ] Keep track of free space per page
    - [ ] Pages with free space
  - [ ] `Page Replacement Policy`:
    - [ ] LRU
    - [ ] Clock Algorithm
    - [ ] LRU-k
  - [ ] General page interface:
    - [ ] Create
    - [ ] Get
    - [ ] Delete
    - [ ] Iterate
  - [ ] Concurrency conntril:
    - [ ] Locks
    - [ ] Latches
- [ ] Durability:
  - [ ] Write Ahead Log
    - [ ] Journal files
- [ ] Compaction
  - [ ] Vacuuming
- [ ] Configuration file:
  - [ ] yaml
  - [ ] conf
- [ ] Page clearance
- [ ] Compression
- [ ] Encoding
- [ ] Versioning
- [ ] Data catalog
  - [ ] Schema definition and storage
  - [ ] Column limits
- [ ] Null support
  - [ ] Bitmaps
- [ ] Index support:
  - [ ] Hash indices
  - [ ] B+Tree
  - [ ] Geo Based Indixes
  - [ ] Vector Index
  - [ ] Radix index
  - [ ] Tries
- [ ] Types support:
  - [ ] Serial for sequences
  - [ ] All int bases
  - [ ] All float types
  - [ ] Booleans
  - [ ] Varchar
  - [ ] BigInt
  - [ ] BigFloat
  - [ ] UUID
  - [ ] Char
  - [ ] Proto
  - [ ] Date
  - [ ] Arrays
  - [ ] Geo type
  - [ ] Vector
  - [ ] Polygon type
  - [ ] Decimal
  - [ ] Money/Currency
  - [ ] Dates type
  - [ ] Vector
- [ ] Consider Object Storages
  - [ ] S3
  - [ ] SlateDB


#### Questions:
- [ ] Columnar vs Row oriented?
  - [ ] An engine per table?
    - [ ] The default enfine being row-oriented
      - [ ] Similar to InnoDB/Block/Row storage format
    - [ ] Support for column oriented structures too
      - [ ] SSTables come here
      - [ ] Think through how compaction works
  - [ ] Column storage choice?
    - [ ] On Disk
    - [ ] Fully on RAM
- [ ] Look into kdb.
- [ ] Support for special structures?
  - [ ] Time based indices
  - [ ] Graph Database structure
  - [ ] Vector indices
    - [ ] Approximate nearest neighbor


#### Storage implementation considerations:
- [ ] Separate the files:
    - [ ] Data file
    - [ ] Index file
- [ ] Consider using deletion markers - `tombstones`.
- [ ] Disks are accessed using system calls
- [ ] The file format must be easy to construct, modify and interpret
- [ ] Garbage collection & fragmentation
- [ ] Binary encoding
- [ ] Endianness:
  - [ ] Big Endian
  - [ ] Little Endian
  - [ ] `RocksDB` has platform-specific definitions that help identify target platform byte order
- [ ] Most numeric data types are represented as fixed-sized values
- [ ] Consider serialization and deserialization
- [ ] Come up with a binary format
- [ ] Consider the `IEEE` standard for the different primitive types.
  - [ ] Like `IEEE 754` for `Floats`
  - [ ] `32-bit` float represents a single-precision value
  - [ ] `Double` represents a double-precision float.
  - [ ] `Pascal` & `German` strings
- [ ] Bit-packed data:
  - [ ] Booleans
  - [ ] Enums
  - [ ] Flags
- [ ] [Bit packing](https://www.databass.dev/links/58)
- [ ] Bitmasks and bitwise operations
- [ ] How addressing is going to be done:
  - [ ] Split into same-size pages; for in-place updates
  - [ ] Single/multiple contiguous blocks
- [ ] File header & footer 
  - [ ] Fixed size
  - [ ] Quick to access 
- [ ] Create a file format
  - [ ] Expose an API for interaction with it
  - [ ] Perform benchmarks
  - [ ] Choose an appropriate language
    - [ ] Rust
    - [ ] C
  - [ ] Probably NSM based (row oriented)
    - [ ] Come up with a name for it
      - [ ] Apricot
    - [ ] Inspirations:
      - [ ] MySQL
        - [ ] Barracuna
        - [ ] Antelope
      - [ ] Postgres
        - [ ] Heap File
      - [ ] SQLite
        - [ ] B-Tree files

#### File Formart Design Decisions
- [ ] File Meta-Data
  - Self-contained to increase portability. COntain all in4 to interpret contents without external dependencies
  - Each file maintains glibal meta-data, usually on its footer, about its contents:
    - Table Schema (Thrift, Protobuf)
    - Row Group offsets/length
    - Tuple counts/ zone maps
- [ ] Format layout
  - PAX model with row groups and column chunks
  - Size of row groups varies and makes compute/memory trade-offs.
- [ ] Type system
  - Physical - low-level byte rep (IEEE-74)
  - Logical - auxilliary types that map to physical types
  - ORC has a more complete set of physical types
- [ ] Encoding schemes
  - How the format stores bytes for contiguous/related data
  - Dictionary encoding
  - Run-Length Encoding
  - Bit-packing
  - Delta encoding
  - Frame of reference
- [ ] Block Compression
  - Avoid this in general
- [ ] Filters
- [ ] Nested Data
  - [ ] Shredded columns
- [ ] For variable length data:
  - [ ] Postgres TOAST
    - [ ] Created per table
    - [ ] `Toast_tuple_threshold` - 2KB (by default)
      - If a tuple hits the 2KB threshold, it's toasted
    - [ ] Compression may yield success
    - [ ] Use a `Toast table` with a pointer from the main table
    - [ ] `PLAIN` mechism:
      - [ ] No compression 
      - [ ] No toast table (out of line storage)
      - [ ] Show the strategy when storin the table
    - [ ] `MAIN` mechanism:
      - [ ] Compression allowed
      - [ ] No toast tbale.
        - [ ] In some cases, it will.
      - [ ] The storage strategy is per column
    - [ ] `EXTENDED` mechanism:
      - [ ] Both compression & toastable are allowed
      - [ ] Include a way to see the actual length of the column vs the stored length
        - [ ] This shades more light into whether compression was used or not.
      - [ ] Compression doesn't always lead to a toast table
      - [ ] In PG, the limit to hit the toast table is 2000 bytes.
    - [ ] The toastable columns stores values in 2kb pages.
      - [ ] And multiples of 2KB. 
    - [ ] `EXTERNAL` mechanism:
      - [ ] No Compression
      - [ ] Only toast possible.
      - [ ] Used when compression has an overhead.
    - [ ] `Toasting internal algorithm`:
      - [ ] `toast_max_chunk_size`
      - [ ] Start with the largest external and extended fields
      - [ ] If row still doesn't fit, try other external and extended fields
      - [ ] Then try to compress main fields
      - [ ] If size > 2KB, send main to toast table
      - [ ] Code in `heaptoast.c`
    - [ ] Toast tables have a performance cost.
    - [ ] `pglz` and `lz4` compression strategies used.


#### Buffer Pool

The storage layer of this storage engine is handles the  `buffer pool manager`. The actual storage layer, the file structure & layout, is implemented by [Apricot](https://github.com/Ochibobo/apricot/tree/master), a `row-oriented storage structure` implemented in `C`. It will expose an API through which this buffer pool will interact with the underlying file system. 


#### OS Directives
Used when storage engine relies on `mmap`.
`madvise`
`mlock`
`msync`

Research: Inform the Database system the memory size it can use on startup. 
Don't run another system on your database machine.

A `frame` is a memory location where we put `pages`.
Page directory: all frames that exist.
Page table: mapping from a  page id to a frame.

Buffer replacement policies:
- Page eviction; this is basically caching.
- Cater for:
  - Speed
  - Accuracy
  - Correctness
  - Metadata overload handling
- LRU
- Clock
  - Linux has a multi-hand clock
  - Faster than LRU
- Susceptible to `sequential flooding` (LRU & Clock)
- LFU
  - Logarithmic complexity relative to cache size
  - Ignores time and accumulates stale pages with high frequency counts that may no longer be relevant
- LRU-K is appropriate
  - Used in SQL-Server, Postgres earlier versions
  - Maintain `ghost cache`
  - MySQL uses approximate LRU-K
- Adaptive Replacement Policy
  - Used in Postgres, DB2, ZFS
- Consider `localization`
- Use `priority hints`
  - Hint a page's importance to the buffer pool
- Evict clean pages; dirty pages are to be written to disk (background writing/ page cleaning/ buffer flushing)
  - After flushing; page can be evicted
- Don't write dirty pages before their logs are written
- Re-order and batch I/O requests
- Linux has deadline or noop(FIFO) scheduler
- DBMS maintains internal queues to track read/write requests from the entire system
- Compute priorities based on several factors:
  - Sequential vs Random I/O.
  - Critical Path task vs Background task
  - Table vs Index vs Log vs Ephemeral Data
  - Transaction information
  - User Based SLA
- Bypass OS cache using `O_DIRECT`
  - Redundant page copies
  - Different eviction policies
  - Loss of control over file I/O
  - Can use DataPlant/DataKit (DpDk)
- Postgres18: accelerate disk reads with async I/Os
- Fsync errors
- Record IDs can be:
  - File Id, Page Id, Slot #
- Maximum number of columns:
  - 216 postgres
  - 1000 Oracle
  - Tuple organized storage
  - Index organized storage
  - HDFS, Google Colossus, S3 Express; can't do in-place updates; only appends.
  - 

#### Storage Engine
- File Manager
- Buffer Pool Manager
- Transaction Manager
  - Lock Manager
  - Write Batch Manager
    - Buffering
    - Atomicity
    - Memory Tracking
      - Track how much memory pending changes are consuming. If a transaction becomes too large, the manager can be configured to reject further writes to prevent the system from running out of RAM
  - Snapshot Manager
    - Responsible for `I`solation; ensures that a transaction sees a consistent version of the data, even if other background processes are modifying the database
    - Sequence Numbers - every single write is tagged with a globally increasing 64-bit sequence number ($seq$)
    - Point-In-Time View - When a snapshot is created, the Snapshot Manager records the current highest sequence number
    - Filtering - When the transaction performs a fetch or an iterator scan, the Snapshot Manager filters out any data with a seq > 100. Even if another thread writes a new value at seq = 101, your transaction will ignore it, seeing the database exactly as it looked the moment the snapshot was taken.
    - Resource Management - The manager keeps track of "active" snapshots. Do not delete old versions of a key if an active snapshot still needs them to provide a consistent view.
  - WAL
  - The Wait-For Graph
    - The Graph: The engine maintains a "Wait-For" graph where nodes are transactions and edges represent one transaction waiting for another to release a lock.
    - The Cycle: A deadlock exists if, and only if, there is a cycle in this graph (e.g., Transaction A waits for B, and Transaction B waits for A).
    - The Kill: Once a cycle is detected, the Transaction Manager must intervene. It picks a "victim"—usually the transaction that has done the least amount of work—and rolls it back (aborts it) to break the cycle.
  - Lock Timeouts
  - It is worth noting that LSM-tree engines like RocksDB experience deadlocks much less frequently. Because they are "append-only" and use sequence numbers for versions, they don't have to lock existing data pages to update them.
- Index Manager
- Page Writer
  - Dedicated background processes that manage the movement of data between RAM and Disk.
  - Background Writer: Continually pushes "dirty" pages to disk to ensure there is always free space in the Buffer Pool for new data.
  - Checkpointer: Periodically ensures that all modified data up to a certain point is written to the physical data files, allowing the WAL to be recycled.
- MVCC Manager/ Undo Log
- Recovery Manager
  - ARIES : Algorithm for Recovery and Isolation Exploiting Semantics.
  - Phase 1: Analysis
    - The engine scans the Write-Ahead Log (WAL) from the last known "Checkpoint" to the end of the file.
    - Identify which transactions were still "In Progress" and which data pages were "Dirty" (modified in RAM but not yet on disk) at the moment of the crash
  - Uses the checkpoint
    - Freeze/Flush: The engine identifies all modified pages in the Buffer Pool (for NSM) or the current MemTable (for LSM).
    - Sync to Disk: These pages are flushed to the data files or SSTables/ Disk Files
    - Log Marking: A special "Checkpoint Record" is written to the WAL. This record says: "Everything before this point is safely on the permanent disk."
    - Pruning: Once the checkpoint is complete, the engine can delete or recycle the old WAL files that came before the checkpoint, saving disk space.
  - Phase 2: Redo (Repeat History)
    - The engine "replays" every single operation found in the WAL that happened after the last checkpoint.
    - Bring the database state exactly to where it was the microsecond before the power failed. Even if a transaction eventually crashed, its changes are "Redone" here to ensure the physical disk matches the log's history.
  - Phase 3: Undo (Rollback)
    - The engine looks at the list of transactions that never reached a "Commit" status in the log
    - It reverses their changes. In an NSM engine, it uses Undo Logs to restore the old values. In an LSM engine like RocksDB, it simply ignores those entries in the MemTable or writes "Tombstones" to cancel them out
  - Modern databases try to avoid "Sharp Checkpoints" because they freeze the system while data is being written. Instead, they use Fuzzy Checkpointing:
    - Sharp Checkpoint: Stops all incoming writes, flushes everything, then resumes. This causes a "spike" in latency where the database seems to hang.
    - Fuzzy Checkpoint: The engine slowly flushes dirty pages in the background while still allowing new transactions. The Checkpoint Record in the log then points to a "range" of time rather than a single frozen moment.